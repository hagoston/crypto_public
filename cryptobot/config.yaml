# Cryptostore sample config file

# Redis or Kafka are required. They are used to batch updates from cryptofeed and the storage medium of choice
#
# del_after_read: (redis only) toggles the removal of data from redis after it has been processed with cryptostore.
# retention_time: (redis only) if data removal is enabled (via del_after_read) will allow retention of data in redis for N seconds.
# socket: (redis only) allows redis connections via a unix domain socket
# start_flush: toggles if redis/kafka should be flushed at the start. Primarily for debugging, it will flush ALL of redis/kafka
cache: redis
redis:
    ip: '127.0.0.1'
    port: 6379
    socket: null
    del_after_read: true
    retention_time: null
    start_flush: true

# Data sources and data types configured under exchanges. Exchange names follow the naming scheme in cryptofeed (they
# must be capitalized) and only exchanges supported by cryptofeed are supported.
# Data types follow cryptofeed definitions, see defines.py in cryptofeed for more details, common ones are
# trades, l2_book, l3_book, funding, ticker, and open_interest
# Trading pairs for all exchanges (except BitMEX, Deribit and other derivatives) follow the currency-quote format.
# Information about what an exchange supports (pairs, data types, etc) can be found via the info() merhod available on
# all cryptofeed exchange clases.
#
# max_depth controls the size of the book to return. The top N levels will be returned, only when those N levels
# have changed.
# book_delta enables book deltas (snapshot, then only deltas are delivered). Snapshops are delivered
# every book_interval updates. book_interval defaults to 1000 if not specified
#
# Retries are the number of times the connection to the exchange will be retried before exiting. -1 is infinity.
# Using a retry of -1 with a bug in your config can lead to bans by exchanges
#
# Channel timeouts are channel specific and control how long a channel can go with no messages before declaring the connection
# dead. Cryptofeed will restart the connection if no message is received in this time period. Default is 120 seconds. -1 means no timeout.
#
# snapshot_interval controls how often to deliver snapshots on NON delta feeds. Book updates between the snapshots are discarded.
# In other words, enabling snapshot_interval and setting it to a value (say 1000) will cause every 1000th book update to be stored (the full book). Other
# updates will be dropped. Can also set up book_depth with snapshot_interval enabled.

exchanges:
    BINANCE_FUTURES:
        retries: -1
        l2_book:
            symbols: [BTC-USDT]
            book_delta: true
            book_interval: 18000 # Number of updates between snapshots - 30min @ 100ms update interval
        trades: [BTC-USDT]

#    BITMEX:
#        channel_timeouts:
#            l2_book: 30
#            trades: 120
#            ticker: 120
#            funding: -1
#        retries: -1
#        l2_book:
#            symbols: [BTC-USD]
#            max_depth: 10
#            book_delta: true
#            book_interval: 100000
#        trades: [BTC-USD]
#        ticker: [BTC-USD]
#        funding: [BTC-USD]
#    COINBASE:
#        retries: -1
#        l3_book:
#            symbols: [BTC-USD]
#            book_delta: true
#            book_interval: 100000
#        trades: [BTC-USD, ETH-USD, ETH-BTC]
#        ticker: [BTC-USD]

# Where to store the data. Currently arctic, influx, elastic, and parquet are supported. More than one can be enabled
storage: [arctic]
# These two values configure how many times to retry a write (before dying)
# and how long to wait between attempts to write after a failure is encountered.
storage_retries: 5
storage_retry_wait: 30

# Configurable passthrough for data - data will be sent in realtime (no aggregation in redis).
# To disable, remove
#pass_through:
#    type: zmq
#    host: '127.0.0.1'
#    port: 5678

influx:
    host: 'http://127.0.0.1:8086'
    db: crypto
    org: org
    create: false
    token: <your token>
    username: <your name>
    password: cryptodb

# arctic specific configuration options - the mongo URL
arctic: mongodb://127.0.0.1

# Data batching window, in seconds, or optionally a time interval: M(inutely), H(ourly), D(aily).
# These intervals will require that you have enough memory available to redis (and python) to hold this amount of data.
# String intervals can be combined with numbers, eg 2H is 2 hours, 5M is 5 minutes, etc.
# Note that if a time interval is selected and kafka is used, the timestamps used to aggregate the data
# for the time intervals from from Kafka's internal timestamp offsets - these will differ from the exchange
# provided timestamps - it may be anywhere from 0.001 ms to seconds depending on your kafka cluster and
# other hardware and setup configurations.
storage_interval: 60
